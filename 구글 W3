import numpy as np
import matplotlib.pyplot as plt
from testCases_v2 import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

#%matplotlib inline
np.random.seed(1)# set a seed so that the results are consistent

#데이터 셋
X,Y=load_planar_dataset()

#시각 데이터
plt.scatter(X[0,:],X[1,:],c=Y,s=40,cmap=plt.cm.Spectral);
#shape로 배열을 표시한다.
shape_X=X.shape
shape_Y=Y.shape

print('the shape of x is '+str(shape_X))
print('the shape of y is '+str(shape_Y))
print('the shape of x is '%(m))

#적합한 로지스틱 회귀 확인 sklearn 회귀 분류기 훈련
clf=sklearn.linear_model.LogisticRegressionCV();
clf.fit(X.T,Y.T);

plot_decision_boundary(lambda x:clf.predict(x),X,Y)
plt.title('Logistic Regression')

LR_predictions=clf.predict(X.T)
print('Accuracy of logistic regression: %d'%float((np.dot(Y,LR_predictions)+np.dot(1-Y,1-LR_predictions))/float(Y.size)*100)+'%'+
      "(percentage of correctily labelled datapoints)")


def layer_sizes(X,Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """
    n_x=X.shape[0] #입력층 개수
    n_h=4
    n_y=Y.shape[0]
    return n_x,n_h,n_y

X_assess, Y_assess = layer_sizes_test_case()
(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)
print("The size of the input layer is: n_x = " + str(n_x))
print("The size of the hidden layer is: n_h = " + str(n_h))
print("The size of the output layer is: n_y = " + str(n_y))

#처음 시도 랜덤 값으로 초기화 bias는 0으로 초기화 Back Propagation에 사용될 값들은 "cache"에 저장됩니다. cache는 이후 구현할 back propagation 함수의 인자로 들어갈 예정입니다
def initialize_parameters(n_x,n_h,n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    params -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """
    np.random.seed(2)
    W1=np.random.randn(n_h,n_x) *0.01
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.randn(n_y,n_h)*0.01
    b2=np.zeros(shape=(n_y,1))
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))

    parameters={'W1':W1,
                'b1':b1,
                'W2':W2,
                'b2':b2}
    return parameters

n_x,n_h,n_y =initialize_parameters_test_case()

parameters=initialize_parameters(n_x,n_h,n_y)

print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))

def forward_propagation(X,parameters):
    """
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
    """
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    
    Z1=np.dot(W1,X)+b1
    A1=np.tanh(Z1)
    Z2=np.dot(W2,X)+b2
    A2=sigmoid(Z2)
    
    assert(A2.shape ==(1,X.shape[1]))
    
    cache={'Z1':Z1,
           'A1':A1,
           'Z2':Z2,
           'A2':A2} #back propagation인자에 들어갈 값
    
    return A2,cache
    
X_assess,parameters= forward_propagation_test_case()
A2,cache=forward_propagation(X_assess,parameters)

print(np.mean['Z1'],np.mean(cache['A1'],np.mean(cache['Z2'],np.mean(cache['A2']))))
#여러분은 np.multiply() 함수를 적용한 다음 np.sum()을 사용하거나, 혹은 다이렉트로 np.dot() 함수를 사용할 수 있습니다.

#np.multiply() 다음에 np.sum() 을 사용하면 최종 결과는 float 유형이 되고, np.dot() 을 사용하면 결과는 2차원 numpy array가 됩니다.
def compute_cost(A2,Y,parameters):
  
    #Computes the cross-entropy cost given in equation (13)
    
    #Arguments:
    #A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    #Y -- "true" labels vector of shape (1, number of examples)
    #parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    #[Note that the parameters argument is not used in this function, 
   # but the auto-grader currently expects this parameter.
   # Future version of this notebook will fix both the notebook 
   # and the auto-grader so that `parameters` is not needed.
   # For now, please include `parameters` in the function signature,
   # and also when invoking this function.]
    
    #Returns:
   # cost -- cross-entropy cost given equation (13)
        
    m=Y.shape[1]
    logprobs=np.multiply(np.log(A2,Y)+np.multiply((1-Y),np.log(1-A2)))
    cost=-np.sum(logprobs)/m #np.dot 사용 2차원 어레이가 된다.
     
    cost=float(np.squeeze(cost)) #np.squeeze 중복된 값을 제거한다.
    
    assert(isinstance(cost,float))
    return cost

A2, Y_assess, parameters = compute_cost_test_case()
print('cost='+str(compute_cost(A2,Y_assess,parameters())))

def backward_propagation(parameters, cache, X, Y):

#    Implement the backward propagation using the instructions above.
    
#    Arguments:
#    parameters -- python dictionary containing our parameters 
#    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
#    X -- input data of shape (2, number of examples)
#    Y -- "true" labels vector of shape (1, number of examples)
    
#    Returns:
#    grads -- python dictionary containing your gradients with respect to different parameters
   
    
    m=X.shape[1]
    
    W1=parameters['W1']
    W2=parameters['W2']
    
    A1=cache['A1']
    A2=cache['A2']
    
    dZ2 = A2-y
    dW2 = 1/m*np.dot(dZ2,A1.T)
    db2 = 1/m*np.sum(dZ2,axis=1,keepdims=True)
    dZ1 = np.multiply(np.dot(W2.T,dZ2),1-np.power(A1,2)) #*=np.multiply
    dW1 = 1/m*np.dot(dZ1,X.T)
    db1 = 1/m*np.sum(dZ1,axis=1,keepdims=True)
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads
parameters, cache, X_assess, Y_assess=backward_propagation_test_case()

grads=backward_propagation(parameters, cache, X_assess, Y_assess)

print('dw1='+str(grads['dw1']))
print('db1='+str(grads['db1']))
print('dw2='+str(grads['dw2']))
print('db2='+str(grads['db2']))

#경사 하강법을 사용합니다. 다시 말해 (W1, b1, W2, b2)를 업데이트하려면 (dW1, db1, dW2, db2)를 사용해야 합니다.
#update는 파라미터
def update_parameters(parameters, grads, learning_rate=1.2):
    #Updates parameters using the gradient descent update rule given above
    
    #Arguments:
    #parameters -- python dictionary containing your parameters 
   # grads -- python dictionary containing your gradients 
    
   # Returns:
   # parameters -- python dictionary containing your updated parameters     
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    
    dW1 = grads['dW1']
    db1 = grads['db2']
    dW2 = grads['dW2']
    db2 = grads['db2']
    
    W1 = W1 -learning_rate*dW1
    b1 = b1- learning_rate*db1
    W2 = W2- learning_rate*dW2
    b2 = b2 -learning_rate*db2
    
    parameters={'W1':W1,
                'b1':b2,
                'W2':W2,
                'b2':b2}
    
    return parameters
    

#return
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    np.random.seed(3)
    n_x=layer_sizes(X,Y)[0] #처음
    n_y=layer_sizes(X,Y)[2] #끝
    parameters=initialize_parameters(n_x,n_h,n_y)
    #loop gradient update
    for i in range(0,num_iterations):
       A2,cache= forward_propagation(X,parameters)
       cost=compute_cost(A2,Y,parameters)
       grads=backward_propagation(parameters, cache, X, Y)
       parameters = update_parameters(parameters, grads)
       if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
    return parameters
        
X_assess, Y_assess = nn_model_test_case()
parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)   
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))    


 # Using the learned parameters, predicts a class for each example in X
    
    #Arguments:
    #parameters -- python dictionary containing your parameters 
   # X -- input data of size (n_x, m)
    
 #   Returns
#    predictions -- vector of predictions of our model (red: 0 / blue: 1)
#Predictions forward
def predict(parameters,X):
    A2,cache=forward_propagation(parameters, X_assess)
    predictions=A2>0.5
    return predictions

parameters, X_assess= predict_test_case()
predictions = predict(parameters, X_assess)
print('predictions maen='+str(np.mean(predictions)))
# Build a model with a n_h-dimensional hidden layer
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)
# Plot the decision boundary
plot_decision_boundary=(lambda x: predict(parameters,x.T),X,Y)
plt.title("Decision Boundary for hidden layer size " + str(4))

# Print accuracy j
predictions = predict(parameters, X)
print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')

#Tuning hidden layer size (optional/ungraded exercise)
plt.figure(figsize=(16.32))
hidden_layer_sizes=[1,2,3,4,5,20,50]
for i,n_h in enumerate(hidden_layer_xizes):
     plt.subplot(5,2,i+1)
     plt.title('hidden %d'%n_h)
     parameters=nn_model(X,Y,n_h,num_iterations=5000)
     plot_decision_boundary(lambda x:predicti(parameters,x.T),X,Y)
     predictions=predict(parameters,X)
     accuracy=float((np.dot(Y,predictions.T)+np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
     print('accuracy for {} hidden units:{}%'.format(n_h,accuracy))
