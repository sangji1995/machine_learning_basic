import pandas as pd
# 붓꽃 데이터 불러오기
df=pd.read_csv('https://archive.ics.uci.edu/ml/''machine-learning-databases/iris/iris.data',header=None)
df.tail() #데이터 끝부분만 출력

import matplotlib.pyplot as plt
import numpy as np
X=df.iloc[0:100,[0,2]].values #앞 100개 샘플의 컬럼0(꽃받침 길이)과 컬럼2(꽃잎 길이) 값을 가지고 옴

#산점도를 그립니다. x축:컬럼0(꽃받침 길이), y축:컬럼1(꽃잎 길이)
plt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa') #setosa에 해당하는 샘플(0~49) 플롯팅
plt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor') #setosa에 해당하는 샘플(50~99) 플롯팅

#그래프 텍스트 설정
plt.xlabel('Sepal length[cm]')
plt.ylabel('petal length[cm]')
plt.legend(loc='upper left')
plt.show()
#class 퍼셉트론
class Perceptron(object):
    def __init__(self, lr=0.01, n_iter=50, random_state=1): #n_iter 학습업데이트
        self.lr=lr #learning rate (학습률,0.0~1.0)
        self.n_iter=n_iter #훈련 데이터셋 반복 횟수
        self.random_state=random_state #난수발생기 시드
    #입력값에 대해 정답 추론
    def predict(self,X):
        output =np.dot(X,self.w_[1:])+self.w_[0]
        return np.where(output>=0.0,1,-1) #출력값이 0 이상이면 1, 아니면 -1을 리턴함
    
    def fit(self,X,y): #업데이트 할거 
        #x:학습 데이터
        #y: 정답값(레이블)
        regn=np.random.RandomState(self.random_state)
        self.w_=regn.normal(loc=0.0, scale=0.01, size=1 +X.shape[1]) #가중치 랜덤 초기화
        self.errors_=[] #에포크마다 누적된 분류 오류 
        
        for _ in range(self.n_iter): #지정한 회수(n_iter)만큼 반복
            errors=0
            for xi, target in zip(X,y): #각 샘플에 대해 반복
                update =self.lr *(target -self.predict(xi))
                self.w_[1:]+=update*xi #가중치 업데이트
                self.w_[0]+=update #바이어스 업데이트
                errors +=int(update !=0.0) #에러 게산
            self.errors_.append(errors)
        return self
    
#퍼셉트론 객체 생성
ppn= Perceptron(lr=0.1, n_iter=10)

#setosa와 versicolor를 선택
y=df.iloc[0:100,4].values
y=np.where(y =='iris-setosa',-1,1)

#학습 실행
#x:앞 100개 샘플의 컬럼0(꽃받침 길이)과 컬럼2(꽃잎 길이) 값, 앞 셀에서 'X=df.iloc[0:100,[0,2]].values로 생성
#y: 앞 100개 샘플의 정답값 (꽃종류). -1 이면 setosa, 1이면 veriscolor.
ppn.fit(X,y)

plt.plot(range(1,len(ppn.errors_)+1),ppn.errors_,marker='o')
plt.xlabel('Epoches')
plt.ylabel('NUmber of errors')
plt.show()

from matplotlib.colors import ListedColormap

def plot_decision_regions(X,y, classifier, resolution=0.02): #간격을 0.02로 쪼갠다
    
    #마커와 컬러맵을 설정
    markers =('s','x','o','^','v')    
    colors=('red','blue','lightgreen','gray','cyan')
    cmap=ListedColormap(colors[:len(np.unique(y))])
    
    #결정 경게를 그립니다.
    x1_min,x1_max = X[:,0].min()-1,X[:,0].max()+1 #한 그래프에서 맨처음 맨끝에 위치하고 그리드로 영역을 쪼갠다. 각각의 부위에 입력값으로 
    #생각 퍼셉트론으로 생각 -1 or 1로 판단 색깔을 다르게 칠한다
    x2_min,x2_max = X[:,1].min()-1,X[:,1].max()+1
    xx1, xx2=np.meshgrid(np.arange(x1_min,x1_max,resolution),np.arange(x2_min,x2_max,resolution))
    Z=classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T) #그리드 모든 점들을 표시한다.
    Z=Z.reshape(xx1.shape)
    plt.contourf(xx1,xx2,Z,alpha=0.3,cmap=cmap)
    plt.xlim(xx1.min(),xx1.max())
    plt.ylim(xx2.min(),xx2.max())
    
    #샘플의 산점도 그립니다.
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y==cl,0],
                    y=X[y==cl,1],
                    alpha=0.8,
                    c=colors[idx],
                    marker=markers[idx],
                    label=cl,
                    edgecolor='black')
        
plot_decision_regions(X,y,classifier=ppn)
plt.xlabel('sepal length [cm]')    
plt.ylabel('petal length [cm]')    
plt.legend(loc='upper left')
plt.show()
